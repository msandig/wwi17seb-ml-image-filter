{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random \n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available 1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23552 images belonging to 23 classes.\n",
      "Found 5888 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "# path to dataset\n",
    "directory = \"./FACD_image\"\n",
    "target_size = (224,224)\n",
    "seed = 42;\n",
    "batch_size=10\n",
    "\n",
    "# create a image generator for keras, that can load images batchwise\n",
    "data_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input, validation_split=0.2)\n",
    "\n",
    "train_batches = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    directory, data_generator, target_size=target_size, color_mode='rgb', batch_size=batch_size, shuffle=True, seed=seed,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_batches = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    directory, data_generator, target_size=target_size,  color_mode='rgb', batch_size=batch_size, shuffle=True, seed=seed, \n",
    "    subset='validation'\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(224,224,3)), \n",
    "    MaxPool2D(pool_size=(2, 2), strides=2), \n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'), \n",
    "    MaxPool2D(pool_size=(2,2), strides=2), \n",
    "    Flatten(), \n",
    "    Dense(units=23, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 23)                4616215   \n",
      "=================================================================\n",
      "Total params: 4,635,607\n",
      "Trainable params: 4,635,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2356 steps, validate for 589 steps\n",
      "Epoch 1/20\n",
      "2242/2356 [===========================>..] - ETA: 5s - loss: 4.8268 - categorical_accuracy: 0.5335 - precision: 0.6409 - recall: 0.4450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dennis\\anaconda3\\envs\\ML\\lib\\site-packages\\PIL\\TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 19273 bytes but only got 816. Skipping tag 700\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "C:\\Users\\Dennis\\anaconda3\\envs\\ML\\lib\\site-packages\\PIL\\TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5140 bytes but only got 816. Skipping tag 34377\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "C:\\Users\\Dennis\\anaconda3\\envs\\ML\\lib\\site-packages\\PIL\\TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 3144 bytes but only got 816. Skipping tag 34675\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356/2356 [==============================] - 185s 79ms/step - loss: 4.6485 - categorical_accuracy: 0.5383 - precision: 0.6472 - recall: 0.4496 - val_loss: 1.0979 - val_categorical_accuracy: 0.6619 - val_precision: 0.7710 - val_recall: 0.5771\n",
      "Epoch 2/20\n",
      "2356/2356 [==============================] - 187s 79ms/step - loss: 0.8162 - categorical_accuracy: 0.7512 - precision: 0.8288 - recall: 0.6797 - val_loss: 0.8970 - val_categorical_accuracy: 0.7405 - val_precision: 0.8189 - val_recall: 0.6797\n",
      "Epoch 3/20\n",
      "2356/2356 [==============================] - 184s 78ms/step - loss: 0.5961 - categorical_accuracy: 0.8209 - precision: 0.8661 - recall: 0.7734 - val_loss: 0.9769 - val_categorical_accuracy: 0.7317 - val_precision: 0.7865 - val_recall: 0.6883\n",
      "Epoch 4/20\n",
      "2356/2356 [==============================] - 183s 78ms/step - loss: 0.4588 - categorical_accuracy: 0.8605 - precision: 0.8917 - recall: 0.8283 - val_loss: 0.9453 - val_categorical_accuracy: 0.7249 - val_precision: 0.8004 - val_recall: 0.6646\n",
      "Epoch 5/20\n",
      "2356/2356 [==============================] - 187s 79ms/step - loss: 0.3730 - categorical_accuracy: 0.8842 - precision: 0.9059 - recall: 0.8614 - val_loss: 1.0602 - val_categorical_accuracy: 0.7165 - val_precision: 0.7884 - val_recall: 0.6664\n",
      "Epoch 6/20\n",
      "2356/2356 [==============================] - 184s 78ms/step - loss: 0.3216 - categorical_accuracy: 0.8995 - precision: 0.9157 - recall: 0.8827 - val_loss: 1.1432 - val_categorical_accuracy: 0.7077 - val_precision: 0.7715 - val_recall: 0.6646\n",
      "Epoch 7/20\n",
      "2356/2356 [==============================] - 185s 78ms/step - loss: 0.2897 - categorical_accuracy: 0.9062 - precision: 0.9218 - recall: 0.8924 - val_loss: 1.1849 - val_categorical_accuracy: 0.7152 - val_precision: 0.7561 - val_recall: 0.6844\n",
      "Epoch 8/20\n",
      "2356/2356 [==============================] - 184s 78ms/step - loss: 0.2560 - categorical_accuracy: 0.9135 - precision: 0.9261 - recall: 0.9025 - val_loss: 1.1790 - val_categorical_accuracy: 0.7194 - val_precision: 0.7645 - val_recall: 0.6850\n",
      "Epoch 9/20\n",
      "2356/2356 [==============================] - 187s 79ms/step - loss: 0.2437 - categorical_accuracy: 0.9167 - precision: 0.9279 - recall: 0.9054 - val_loss: 1.1153 - val_categorical_accuracy: 0.7434 - val_precision: 0.7836 - val_recall: 0.7196\n",
      "Epoch 10/20\n",
      "2356/2356 [==============================] - 184s 78ms/step - loss: 0.2115 - categorical_accuracy: 0.9252 - precision: 0.9334 - recall: 0.9171 - val_loss: 1.3150 - val_categorical_accuracy: 0.7201 - val_precision: 0.7575 - val_recall: 0.6975\n",
      "Epoch 11/20\n",
      "2356/2356 [==============================] - 198s 84ms/step - loss: 0.2085 - categorical_accuracy: 0.9225 - precision: 0.9320 - recall: 0.9142 - val_loss: 1.2707 - val_categorical_accuracy: 0.7291 - val_precision: 0.7644 - val_recall: 0.7047\n",
      "Epoch 12/20\n",
      "2356/2356 [==============================] - 203s 86ms/step - loss: 0.1924 - categorical_accuracy: 0.9282 - precision: 0.9360 - recall: 0.9209 - val_loss: 1.1190 - val_categorical_accuracy: 0.7374 - val_precision: 0.7797 - val_recall: 0.7118\n",
      "Epoch 13/20\n",
      "2356/2356 [==============================] - 201s 85ms/step - loss: 0.1966 - categorical_accuracy: 0.9278 - precision: 0.9352 - recall: 0.9216 - val_loss: 1.2224 - val_categorical_accuracy: 0.7322 - val_precision: 0.7678 - val_recall: 0.7126\n",
      "Epoch 14/20\n",
      "2356/2356 [==============================] - 194s 82ms/step - loss: 0.1807 - categorical_accuracy: 0.9284 - precision: 0.9354 - recall: 0.9203 - val_loss: 1.3952 - val_categorical_accuracy: 0.7167 - val_precision: 0.7483 - val_recall: 0.6968\n",
      "Epoch 15/20\n",
      "2356/2356 [==============================] - 182s 77ms/step - loss: 0.1851 - categorical_accuracy: 0.9272 - precision: 0.9339 - recall: 0.9195 - val_loss: 1.0486 - val_categorical_accuracy: 0.7388 - val_precision: 0.7942 - val_recall: 0.6989\n",
      "Epoch 16/20\n",
      "2356/2356 [==============================] - 181s 77ms/step - loss: 0.1713 - categorical_accuracy: 0.9310 - precision: 0.9375 - recall: 0.9254 - val_loss: 1.2959 - val_categorical_accuracy: 0.7362 - val_precision: 0.7695 - val_recall: 0.7206\n",
      "Epoch 17/20\n",
      "2356/2356 [==============================] - 181s 77ms/step - loss: 0.1705 - categorical_accuracy: 0.9296 - precision: 0.9359 - recall: 0.9228 - val_loss: 1.3241 - val_categorical_accuracy: 0.7018 - val_precision: 0.7411 - val_recall: 0.6797\n",
      "Epoch 18/20\n",
      "2356/2356 [==============================] - 182s 77ms/step - loss: 0.1698 - categorical_accuracy: 0.9306 - precision: 0.9366 - recall: 0.9245 - val_loss: 1.1973 - val_categorical_accuracy: 0.7300 - val_precision: 0.7719 - val_recall: 0.6989\n",
      "Epoch 19/20\n",
      "2356/2356 [==============================] - 204s 87ms/step - loss: 0.1636 - categorical_accuracy: 0.9305 - precision: 0.9356 - recall: 0.9246 - val_loss: 1.2399 - val_categorical_accuracy: 0.7335 - val_precision: 0.7682 - val_recall: 0.7099\n",
      "Epoch 20/20\n",
      "2356/2356 [==============================] - 209s 89ms/step - loss: 0.1630 - categorical_accuracy: 0.9323 - precision: 0.9378 - recall: 0.9258 - val_loss: 1.4234 - val_categorical_accuracy: 0.7065 - val_precision: 0.7499 - val_recall: 0.6773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16bf0e9fe08>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=20, steps_per_epoch=len(train_batches), verbose=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./second_try_dennis.hdf5\"\n",
    "\n",
    "model.save(\n",
    "   filepath, overwrite=True, include_optimizer=True, save_format=\"h5\",\n",
    "    signatures=None, options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
