{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unterscheidung aller Filter mithilfe Keras-Sequential-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der notwendigen Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU-Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available 1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des Datasets\n",
    "\n",
    "Das Datenset FACD_image umfasst 23 Ordner die jeweils 1280 Bilder enthalten, die mit dem gleichen Gimp-Filter bearbeitet wurden. Über den ImageDataGenerator und den Directory können die Bilder bereits beim Laden entsprechend vorbearbeitet werden. Hierfür wird die preprocess-function des VGG16 Modells verwendet. Zusätzlich werden die Bilder auf eine größe von 224x224 Skaliert und mit RBG Farbwerten gespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23552 images belonging to 23 classes.\n",
      "Found 5888 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "# path to dataset\n",
    "directory = \"./FACD_image\"\n",
    "target_size = (224,224)\n",
    "seed = 42;\n",
    "batch_size=10\n",
    "\n",
    "# create a image generator for keras, that can load images batchwise\n",
    "data_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input, validation_split=0.2)\n",
    "\n",
    "train_batches = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    directory, data_generator, target_size=target_size, color_mode='rgb', batch_size=batch_size, shuffle=True, seed=seed,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_batches = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    directory, data_generator, target_size=target_size,  color_mode='rgb', batch_size=batch_size, shuffle=True, seed=seed, \n",
    "    subset='validation'\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellung des Modells\n",
    "\n",
    "Als Modell wird das Sequential Modell von Keras verwendet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 23)                4616215   \n",
      "=================================================================\n",
      "Total params: 4,635,607\n",
      "Trainable params: 4,635,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Definition des Modells\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(224,224,3)), \n",
    "    MaxPool2D(pool_size=(2, 2), strides=2), \n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'), \n",
    "    MaxPool2D(pool_size=(2,2), strides=2), \n",
    "    Flatten(), \n",
    "    Dense(units=23, activation='softmax'),\n",
    "])\n",
    "\n",
    "#Ausgeben einer Zusammenfassung \n",
    "model.summary()\n",
    "\n",
    "\n",
    "#Compilieren und fürs Trainieren Vorbereiten\n",
    "metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainieren des Modells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2356 steps, validate for 589 steps\n",
      "Epoch 1/20\n",
      " 857/2356 [=========>....................] - ETA: 1:21 - loss: 0.5679 - categorical_accuracy: 0.8352 - precision: 0.8794 - recall: 0.7846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dennis\\anaconda3\\envs\\ML\\lib\\site-packages\\PIL\\TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 19273 bytes but only got 816. Skipping tag 700\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "C:\\Users\\Dennis\\anaconda3\\envs\\ML\\lib\\site-packages\\PIL\\TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5140 bytes but only got 816. Skipping tag 34377\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "C:\\Users\\Dennis\\anaconda3\\envs\\ML\\lib\\site-packages\\PIL\\TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 3144 bytes but only got 816. Skipping tag 34675\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356/2356 [==============================] - 206s 88ms/step - loss: 0.6620 - categorical_accuracy: 0.8071 - precision: 0.8589 - recall: 0.7531 - val_loss: 0.9675 - val_categorical_accuracy: 0.7096 - val_precision: 0.8027 - val_recall: 0.6342\n",
      "Epoch 2/20\n",
      "2356/2356 [==============================] - 205s 87ms/step - loss: 0.5115 - categorical_accuracy: 0.8497 - precision: 0.8851 - recall: 0.8142 - val_loss: 0.9466 - val_categorical_accuracy: 0.7306 - val_precision: 0.7994 - val_recall: 0.6858\n",
      "Epoch 3/20\n",
      "2356/2356 [==============================] - 204s 86ms/step - loss: 0.4463 - categorical_accuracy: 0.8667 - precision: 0.8939 - recall: 0.8357 - val_loss: 1.0338 - val_categorical_accuracy: 0.7142 - val_precision: 0.7757 - val_recall: 0.6743\n",
      "Epoch 4/20\n",
      "2356/2356 [==============================] - 197s 83ms/step - loss: 0.3477 - categorical_accuracy: 0.8942 - precision: 0.9135 - recall: 0.8747 - val_loss: 1.1243 - val_categorical_accuracy: 0.7191 - val_precision: 0.7657 - val_recall: 0.6860\n",
      "Epoch 5/20\n",
      "2356/2356 [==============================] - 189s 80ms/step - loss: 0.3183 - categorical_accuracy: 0.9002 - precision: 0.9171 - recall: 0.8822 - val_loss: 1.2580 - val_categorical_accuracy: 0.7043 - val_precision: 0.7569 - val_recall: 0.6647\n",
      "Epoch 6/20\n",
      "2356/2356 [==============================] - 188s 80ms/step - loss: 0.2828 - categorical_accuracy: 0.9060 - precision: 0.9205 - recall: 0.8902 - val_loss: 1.2199 - val_categorical_accuracy: 0.6990 - val_precision: 0.7599 - val_recall: 0.6546\n",
      "Epoch 7/20\n",
      "2356/2356 [==============================] - 208s 88ms/step - loss: 0.2467 - categorical_accuracy: 0.9172 - precision: 0.9296 - recall: 0.9048 - val_loss: 1.1703 - val_categorical_accuracy: 0.7133 - val_precision: 0.7664 - val_recall: 0.6775\n",
      "Epoch 8/20\n",
      "2356/2356 [==============================] - 203s 86ms/step - loss: 0.2371 - categorical_accuracy: 0.9203 - precision: 0.9306 - recall: 0.9096 - val_loss: 1.3470 - val_categorical_accuracy: 0.6940 - val_precision: 0.7353 - val_recall: 0.6608\n",
      "Epoch 9/20\n",
      "2356/2356 [==============================] - 201s 85ms/step - loss: 0.2217 - categorical_accuracy: 0.9226 - precision: 0.9327 - recall: 0.9124 - val_loss: 1.4718 - val_categorical_accuracy: 0.6936 - val_precision: 0.7404 - val_recall: 0.6661\n",
      "Epoch 10/20\n",
      "2356/2356 [==============================] - 199s 84ms/step - loss: 0.2131 - categorical_accuracy: 0.9237 - precision: 0.9320 - recall: 0.9147 - val_loss: 1.5628 - val_categorical_accuracy: 0.6809 - val_precision: 0.7228 - val_recall: 0.6557\n",
      "Epoch 11/20\n",
      "2356/2356 [==============================] - 202s 86ms/step - loss: 0.2033 - categorical_accuracy: 0.9266 - precision: 0.9345 - recall: 0.9188 - val_loss: 1.2360 - val_categorical_accuracy: 0.7152 - val_precision: 0.7559 - val_recall: 0.6826\n",
      "Epoch 12/20\n",
      "2356/2356 [==============================] - 203s 86ms/step - loss: 0.1956 - categorical_accuracy: 0.9272 - precision: 0.9347 - recall: 0.9189 - val_loss: 1.4748 - val_categorical_accuracy: 0.6940 - val_precision: 0.7444 - val_recall: 0.6613\n",
      "Epoch 13/20\n",
      "2356/2356 [==============================] - 202s 86ms/step - loss: 0.1947 - categorical_accuracy: 0.9249 - precision: 0.9321 - recall: 0.9174 - val_loss: 1.4630 - val_categorical_accuracy: 0.6863 - val_precision: 0.7284 - val_recall: 0.6600\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "validation_freq = 1\n",
    "filepath = \"./second_try_dennis_checkpointsave.hdf5\"\n",
    "\n",
    "es=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta=0.002, patience=0, verbose=0, mode='min',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")\n",
    "\n",
    "mc=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=0, save_best_only=False,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_batches, \n",
    "    validation_data=valid_batches, \n",
    "    epochs=epochs, \n",
    "    steps_per_epoch=len(train_batches), \n",
    "    validation_freq=validation_freq,\n",
    "    verbose=1, \n",
    "    workers=4,\n",
    "    callbacks=[es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotten der Lern und Fehlerkurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ecfd44575e1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_keras_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\wwi17seb-ml-image-filter\\plot.py\u001b[0m in \u001b[0;36mplot_keras_history\u001b[1;34m(history, epochs, metric, vali_freq)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lower right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2761\u001b[0m     return gca().plot(\n\u001b[0;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2763\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1645\u001b[0m         \"\"\"\n\u001b[0;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1647\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1648\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (2,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plot\n",
    "\n",
    "plot.plot_keras_history(history, epochs, metrics[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
